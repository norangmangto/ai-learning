"""
Quick Reference - Alternative LLM Approaches
Run this to see all available implementations
"""


def show_implementations():
    """Display all available implementations"""

    implementations = {
        "1. Hugging Face Transformers": {
            "file": "train_huggingface.py",
            "description": "Local models (DistilGPT2, GPT-2, BERT, BART)",
            "install": "pip install transformers torch",
            "run": "python train_huggingface.py",
            "use_case": "Experimentation and development",
            "difficulty": "â­ Easy"
        },
        "2. Ollama": {
            "file": "train_ollama.py",
            "description": "Local LLM server (Llama2, Mistral, CodeLlama)",
            "install": "curl https://ollama.ai/install.sh | sh",
            "run": "ollama serve && python train_ollama.py",
            "use_case": "Development and prototyping",
            "difficulty": "â­ Easy"
        },
        "3. LangChain": {
            "file": "train_langchain.py",
            "description": "Multi-provider framework with chains and RAG",
            "install": "pip install langchain langchain-community",
            "run": "python train_langchain.py",
            "use_case": "Complex LLM applications",
            "difficulty": "â­â­ Medium"
        },
        "4. Quantized Models": {
            "file": "train_quantized.py",
            "description": "CPU-optimized GGUF models",
            "install": "pip install gpt4all",
            "run": "python train_quantized.py",
            "use_case": "Edge devices and laptops",
            "difficulty": "â­â­ Medium"
        },
        "5. All Approaches": {
            "file": "train_all_approaches.py",
            "description": "Comprehensive demo with benchmarks",
            "install": "See individual approaches above",
            "run": "python train_all_approaches.py",
            "use_case": "Comparison and evaluation",
            "difficulty": "â­ Easy"
        }
    }

    print("=" * 70)
    print("ALTERNATIVE LLM APPROACHES - QUICK REFERENCE")
    print("=" * 70)

    for name, details in implementations.items():
        print(f"\n{name}")
        print("-" * 70)
        print(f"File:        {details['file']}")
        print(f"Description: {details['description']}")
        print(f"Install:     {details['install']}")
        print(f"Run:         {details['run']}")
        print(f"Use Case:    {details['use_case']}")
        print(f"Difficulty:  {details['difficulty']}")

    print("\n" + "=" * 70)
    print("QUICK START GUIDE")
    print("=" * 70)

    print("\n1ï¸âƒ£  Fastest Way to Start (No Setup):")
    print("   python train_huggingface.py")

    print("\n2ï¸âƒ£  Best Quality (Requires Ollama):")
    print("   # Install Ollama from https://ollama.ai")
    print("   ollama pull llama2")
    print("   ollama serve")
    print("   python train_ollama.py")

    print("\n3ï¸âƒ£  For Laptops/CPUs:")
    print("   pip install gpt4all")
    print("   python train_quantized.py")

    print("\n4ï¸âƒ£  For Complex Apps:")
    print("   pip install langchain langchain-community")
    print("   python train_langchain.py")

    print("\n5ï¸âƒ£  Compare All Approaches:")
    print("   python train_all_approaches.py")

    print("\n" + "=" * 70)
    print("RECOMMENDATIONS BY SCENARIO")
    print("=" * 70)

    scenarios = {
        "ğŸ“ Learning ML/AI": "train_huggingface.py",
        "ğŸ’» Building a Prototype": "train_ollama.py",
        "ğŸš€ Production Application": "train_langchain.py",
        "ğŸ“± Running on Laptop": "train_quantized.py",
        "ğŸ”¬ Research & Comparison": "train_all_approaches.py",
        "ğŸ¢ Enterprise (Privacy)": "train_ollama.py",
        "âš¡ Quick Demo": "train_huggingface.py"
    }

    for scenario, file in scenarios.items():
        print(f"{scenario:30} â†’ {file}")

    print("\n" + "=" * 70)
    print("FEATURES COMPARISON")
    print("=" * 70)

    features = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Feature       â”‚   HF     â”‚ Ollama  â”‚ Chain  â”‚ Quantized  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ No Setup Required â”‚    âœ“     â”‚    âœ—    â”‚   âœ—    â”‚     âœ—      â”‚
â”‚ Best Quality      â”‚    ~     â”‚    âœ“    â”‚   âœ“    â”‚     ~      â”‚
â”‚ CPU Friendly      â”‚    ~     â”‚    ~    â”‚   ~    â”‚     âœ“      â”‚
â”‚ Privacy (Local)   â”‚    âœ“     â”‚    âœ“    â”‚   ~    â”‚     âœ“      â”‚
â”‚ Memory Efficient  â”‚    âœ—     â”‚    ~    â”‚   ~    â”‚     âœ“      â”‚
â”‚ Complex Workflows â”‚    âœ—     â”‚    ~    â”‚   âœ“    â”‚     âœ—      â”‚
â”‚ Easy Integration  â”‚    âœ“     â”‚    âœ“    â”‚   ~    â”‚     âœ“      â”‚
â”‚ Multiple Models   â”‚    âœ“     â”‚    âœ“    â”‚   âœ“    â”‚     âœ“      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend: âœ“ = Excellent, ~ = Good, âœ— = Limited
"""
    print(features)

    print("\n" + "=" * 70)
    print("INSTALLATION SUMMARY")
    print("=" * 70)

    print("\n# Minimal (Hugging Face only)")
    print("pip install transformers torch")

    print("\n# Complete (All approaches)")
    print("pip install transformers torch langchain langchain-community "
          "gpt4all")

    print("\n# Ollama (separate installation)")
    print("# macOS/Linux:")
    print("curl https://ollama.ai/install.sh | sh")
    print("# Windows:")
    print("# Download from https://ollama.ai")

    print("\n" + "=" * 70)
    print("For detailed documentation, see README.md")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    show_implementations()
