"""
Code Llama Implementation - Specialized for Code Generation
Meta's LLM optimized for programming tasks
"""

import warnings

warnings.filterwarnings("ignore")


def train():
    print("=== Code Llama Implementation ===\n")

    # 1. Basic Code Llama Setup
    print("1. Testing Code Llama for Code Generation...")
    try:
        from transformers import (
            AutoTokenizer,
            AutoModelForCausalLM,
            pipeline
        )
        import torch

        print("Available Code Llama models:")
        print("- codellama/CodeLlama-7b-hf (Base)")
        print("- codellama/CodeLlama-7b-Instruct-hf (Instruction)")
        print("- codellama/CodeLlama-7b-Python-hf (Python specialized)")
        print("- codellama/CodeLlama-13b-hf")
        print("- codellama/CodeLlama-34b-hf")

        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"\nUsing device: {device}")

        model_name = "codellama/CodeLlama-7b-Instruct-hf"

        print(f"\nLoading {model_name}...")

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            low_cpu_mem_usage=True
        )

        if device == "cpu":
            model = model.to(device)

        print(f"‚úì Code Llama loaded successfully on {device}")

        # Test code generation
        prompt = "Write a Python function to calculate fibonacci numbers:"

        inputs = tokenizer(prompt, return_tensors="pt").to(device)

        print(f"\nPrompt: {prompt}")

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.2,  # Lower for code generation
                top_p=0.95,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        code = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"\nGenerated:\n{code}")

        print("\n‚úì Code generation completed successfully")

    except ImportError:
        print("Install: pip install transformers torch accelerate")
    except Exception as e:
        print(f"Error: {e}")

    # 2. Code Completion (Infilling)
    print("\n2. Testing Code Infilling...")
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch

        device = "cuda" if torch.cuda.is_available() else "cpu"
        model_name = "codellama/CodeLlama-7b-hf"

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        )

        if device == "cpu":
            model = model.to(device)

        # Infilling task
        prefix = "def calculate_sum(a, b):\n    "
        suffix = "\n    return result"

        prompt = f"<PRE> {prefix} <SUF>{suffix} <MID>"

        print(f"Prefix: {prefix}")
        print(f"Suffix: {suffix}")

        inputs = tokenizer(prompt, return_tensors="pt").to(device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.2,
                pad_token_id=tokenizer.eos_token_id
            )

        completion = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"\nCompleted code:\n{completion}")

        print("\n‚úì Code infilling completed successfully")

    except Exception as e:
        print(f"Skipping: {e}")

    # 3. Python-Specific Model
    print("\n3. Testing Code Llama Python...")
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch

        device = "cuda" if torch.cuda.is_available() else "cpu"
        model_name = "codellama/CodeLlama-7b-Python-hf"

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        )

        if device == "cpu":
            model = model.to(device)

        # Python code generation
        prompts = [
            "# Function to read CSV file and return dataframe\n",
            "# Class for a binary search tree\n",
            "# Decorator to measure execution time\n"
        ]

        for prompt in prompts:
            print(f"\nPrompt: {prompt.strip()}")

            inputs = tokenizer(prompt, return_tensors="pt").to(device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,
                    temperature=0.2,
                    top_p=0.95,
                    pad_token_id=tokenizer.eos_token_id
                )

            code = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"Generated:\n{code}\n")

        print("\n‚úì Python-specific generation completed successfully")

    except Exception as e:
        print(f"Skipping: {e}")

    # 4. Instruction Format
    print("\n4. Testing Instruction Format...")
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch

        device = "cuda" if torch.cuda.is_available() else "cpu"
        model_name = "codellama/CodeLlama-7b-Instruct-hf"

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        )

        if device == "cpu":
            model = model.to(device)

        # Instruction-based prompts
        instructions = [
            "Write a Python function to sort a list using quicksort",
            "Create a class for managing a shopping cart",
            "Implement a binary search algorithm"
        ]

        for instruction in instructions:
            prompt = f"[INST] {instruction} [/INST]"

            print(f"\nInstruction: {instruction}")

            inputs = tokenizer(prompt, return_tensors="pt").to(device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.2,
                    top_p=0.95,
                    pad_token_id=tokenizer.eos_token_id
                )

            code = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"Generated:\n{code}\n")

        print("\n‚úì Instruction format completed successfully")

    except Exception as e:
        print(f"Skipping: {e}")

    # 5. Model Comparison
    print("\n5. Code Llama Model Variants...")

    variants = {
        "CodeLlama-7b": {
            "size": "7B",
            "type": "Base",
            "specialty": "General code",
            "use_case": "Code completion, infilling"
        },
        "CodeLlama-7b-Python": {
            "size": "7B",
            "type": "Python",
            "specialty": "Python code",
            "use_case": "Python-specific tasks"
        },
        "CodeLlama-7b-Instruct": {
            "size": "7B",
            "type": "Instruct",
            "specialty": "Instructions",
            "use_case": "Natural language to code"
        },
        "CodeLlama-13b": {
            "size": "13B",
            "type": "Base",
            "specialty": "General code",
            "use_case": "Better quality completion"
        },
        "CodeLlama-34b": {
            "size": "34B",
            "type": "Base",
            "specialty": "General code",
            "use_case": "Highest quality"
        }
    }

    print("\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
    print("‚îÇ         Model           ‚îÇ Size ‚îÇ   Type   ‚îÇ  Specialty   ‚îÇ      Use Case       ‚îÇ")
    print("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")

    for model, info in variants.items():
        print(f"‚îÇ {model:23} ‚îÇ {info['size']:4} ‚îÇ "
              f"{info['type']:8} ‚îÇ {info['specialty']:12} ‚îÇ "
              f"{info['use_case']:19} ‚îÇ")

    print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")

    # 6. Supported Languages
    print("\n6. Supported Programming Languages...")

    languages = [
        "Python", "C++", "Java", "PHP", "TypeScript/JavaScript",
        "C#", "Bash", "SQL", "Go", "Rust", "Ruby", "Swift",
        "Kotlin", "Scala", "Perl", "Lua", "R"
    ]

    print("\nSupported languages:")
    for i in range(0, len(languages), 4):
        row = languages[i:i+4]
        print("  " + ", ".join(f"{lang:15}" for lang in row))

    # 7. Best Practices
    print("\n7. Best Practices for Code Generation...")

    practices = {
        "Temperature": "0.1-0.2 for deterministic code",
        "Max tokens": "256-512 for functions, 1024+ for classes",
        "Comments": "Use comments to guide generation",
        "Context": "Provide function signatures and docstrings",
        "Language": "Specify language in comments",
        "Testing": "Always test generated code"
    }

    for practice, tip in practices.items():
        print(f"- {practice}: {tip}")

    # 8. Example Use Cases
    print("\n8. Example Use Cases...")

    print("\nüìù Code Completion:")
    print("def process_data(df):")
    print("    # Clean missing values")
    print("    # [Model completes the implementation]")

    print("\nüîß Code Explanation:")
    print("[INST] Explain this code: "
          "lambda x: x**2 if x > 0 else 0 [/INST]")

    print("\nüêõ Bug Fixing:")
    print("[INST] Fix the bug in this code: "
          "for i in range(len(arr) + 1): print(arr[i]) [/INST]")

    print("\nüìö Documentation:")
    print("# Generate docstring for this function:")
    print("def calculate_distance(x1, y1, x2, y2):")

    print("\nüîÑ Code Translation:")
    print("[INST] Convert this Python code to JavaScript: "
          "def add(a, b): return a + b [/INST]")

    # QA Validation
    print("\n=== QA Validation ===")
    print("‚úì Code Llama variants explained")
    print("‚úì Code generation demonstrated")
    print("‚úì Code infilling shown")
    print("‚úì Python specialization tested")
    print("‚úì Instruction format documented")
    print("‚úì Supported languages listed")
    print("‚úì Best practices provided")

    print("\n=== Summary ===")
    print("Code Llama Key Features:")
    print("- Specialized for code generation")
    print("- 7B, 13B, 34B parameter models")
    print("- Base, Python, and Instruct variants")
    print("- Supports 17+ programming languages")
    print("- Code infilling capability")
    print("- 100K token context window")
    print("\nRecommendations:")
    print("- Use 7b-Python for Python-specific tasks")
    print("- Use 7b-Instruct for natural language to code")
    print("- Use base models for code completion")
    print("- Use temperature 0.1-0.2 for code generation")
    print("- Always validate and test generated code")

    return {
        "model": "Code-Llama",
        "variants": list(variants.keys()),
        "languages": len(languages),
        "recommended": "CodeLlama-7b-Instruct-hf"
    }


if __name__ == "__main__":
    train()
