# Word Embeddings (Word2Vec, GloVe, FastText)

Dense vector representations of words capturing semantic meaning.

## ğŸ“‹ Overview

**Type:** Unsupervised representation learning
**Dimensions:** Typically 100-300
**Output:** Word â†’ Vector
**Best For:** NLP preprocessing, semantic similarity

## ğŸ¯ Core Idea

Convert words to vectors where similar words are close together.

```
Vector space:
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  king           â”‚
         â”‚    - man        â”‚
         â”‚    + woman  =   queen  âœ“
         â”‚                 â”‚
         â”‚  Paris - France â”‚
         â”‚    + Germany = Berlin  âœ“
         â”‚                 â”‚
         â”‚  Good - Bad     â”‚
         â”‚    + Worse  = Terrible âœ“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Semantic relationships encoded as vector operations!
```

## ğŸ—ï¸ Word2Vec: Skip-gram Model

### Concept
```
Training: Predict context words from target word

Input: "The quick brown fox jumps"
       word="quick"

Predict: ["The", "brown"] (window=1)

Network: Input word â†’ Hidden (embedding) â†’ Output (context)
Result: Learn embeddings that predict context well
```

### Architecture
```
Word index i
     â”‚
     â†“ (one-hot or embedding lookup)
  Embedding layer (d dimensions)
     â”‚
     â†“ (hidden layer)
  Hidden layer (shared for all positions)
     â”‚
     â†“ (linear)
  Output softmax (vocabulary size)
     â”‚
     â†“
Predict context word j
```

## ğŸ“ Word2Vec Mathematics

### Skip-gram Objective
Maximize: $$\sum_{t=1}^{T} \sum_{-m \leq j \leq m, j \neq 0} \log P(w_{t+j} | w_t)$$

Where:
- $w_t$ = target word at position t
- $w_{t+j}$ = context word
- m = window size

### Softmax Probability
$$P(w_j | w_i) = \frac{\exp(v_j^T v_i)}{\sum_{k=1}^{V} \exp(v_k^T v_i)}$$

Where $v_i$ is embedding of word i.

## ğŸ¨ Different Embedding Models

### Word2Vec
```
Skip-gram: Word â†’ Predict context
CBOW: Context â†’ Predict word

Advantages:
âœ“ Fast training
âœ“ Well-understood
âœ“ Produces good embeddings

Disadvantages:
âœ— One embedding per word (polysemy issue)
âœ— Unknown words â†’ special token
```

### GloVe (Global Vectors)
```
Combines:
- Global statistics (like LSA)
- Local context windows (like Word2Vec)

Advantages:
âœ“ Better on small datasets
âœ“ Captures global structure
âœ“ Fast

Disadvantages:
âœ— Still one vector per word
âœ— Requires preprocessing for vocabulary
```

### FastText
```
Words â†’ Character n-grams â†’ Embeddings

Advantages:
âœ“ Handles unknown words (compose from n-grams)
âœ“ Better for morphologically rich languages
âœ“ Useful for rare words

Disadvantages:
âœ— Slower training
âœ— Larger model size
âœ— Still not contextual
```

## ğŸš€ Quick Start

### Word2Vec (Gensim)
```python
from gensim.models import Word2Vec

# Data
sentences = [
    ['the', 'quick', 'brown', 'fox'],
    ['a', 'lazy', 'dog'],
    ['the', 'brown', 'dog']
]

# Train
model = Word2Vec(
    sentences,
    vector_size=100,  # Embedding dimension
    window=5,         # Context window
    min_count=1,      # Ignore words appearing < min_count times
    sg=1              # 1=Skip-gram, 0=CBOW
)

# Embeddings
dog_vector = model.wv['dog']  # (100,)

# Similarity
similarity = model.wv.similarity('dog', 'cat')  # ~0.8

# Most similar
similar_words = model.wv.most_similar('dog', topn=5)
# [('cat', 0.82), ('puppy', 0.79), ...]

# Analogies
result = model.wv.most_similar(positive=['king', 'woman'],
                               negative=['man'], topn=1)
# Should find 'queen'
```

### GloVe
```python
from glove import Corpus, Glove

# Build corpus
corpus = Corpus()
corpus.fit(texts, window=10)

# Train GloVe
glove = Glove(no_components=100, learning_rate=0.05)
glove.fit(corpus.matrix, epochs=30, no_threads=4)

# Embeddings
word_vector = glove.word_vectors[glove.dictionary['dog']]
```

### FastText
```python
from gensim.models import FastText

# Train
model = FastText(
    sentences,
    vector_size=100,
    window=5,
    min_count=1
)

# Embeddings
# Even unknown words get vectors (from n-grams)
unknown_vector = model.wv['unknownword123']
```

## ğŸ“Š Comparing Embeddings

| Aspect | Word2Vec | GloVe | FastText |
|--------|----------|-------|----------|
| **Type** | Predictive | Count-based | Hybrid |
| **Speed** | Fast | Medium | Slow |
| **Unknown words** | Special token | OOV | N-gram compose âœ“ |
| **Morphology** | Poor | Medium | Good âœ“ |
| **Quality** | Good | Good | Good |
| **Small datasets** | Fair | Good âœ“ | Good âœ“ |

## ğŸ¯ Applications

| Task | Best Model |
|------|-----------|
| **General NLP** | Word2Vec (standard) |
| **Morphological** | FastText (inflections) |
| **Small data** | GloVe |
| **Similarity** | All work well |
| **Analogy** | Word2Vec |

## âš ï¸ Important Limitations

```
Static embeddings: One vector per word

Problem: Polysemy (multiple meanings)
Example: "bank" (financial institution vs riverbank)
Solution: Contextual embeddings (BERT, GPT)

Problem: No context
Example: Same embedding for "good" always
         Ignores sentiment context
Solution: Contextual embeddings

These methods became less common with transformers
But still useful for:
- Quick baselines
- Feature engineering
- Lightweight models
```

## ğŸ“ Learning Outcomes

- [x] Skip-gram and CBOW training
- [x] Embedding visualization
- [x] Similarity and analogy tasks
- [x] Different embedding types
- [x] Word vs contextual embeddings

## ğŸ“š Key Papers

- **Word2Vec**: "Efficient Estimation of Word Representations" (Mikolov et al., 2013)
- **GloVe**: "GloVe: Global Vectors for Word Representation" (Pennington et al., 2014)
- **FastText**: "Enriching Word Vectors with Subword Information" (Bojanowski et al., 2017)

## ğŸ’ª Advantages

âœ… **Fast training** - Minutes on CPU
âœ… **Well-understood** - Decades of research
âœ… **Interpretable** - Vector operations show relationships
âœ… **Lightweight** - Small memory footprint
âœ… **Widely available** - Pretrained models everywhere

## ğŸš¨ Disadvantages

âŒ **Not contextual** - Same vector regardless of usage
âŒ **Polysemy** - Can't distinguish multiple meanings
âŒ **Static** - Fixed for all tasks
âŒ **Vocabulary** - Needs coverage of words
âŒ **Limited info** - Doesn't capture fine-grained semantics

## ğŸ’¡ Modern Perspective

```
These methods (Word2Vec, GloVe, FastText) are foundational
but largely superseded by contextual embeddings:

âœ“ Historical importance: Very high
âœ“ Modern use: Lower (but still used for features)
âœ“ Learning value: Essential for understanding NLP

Progression:
Word2Vec (2013) â†’ ELMo (2018) â†’ BERT/GPT (2018+)
```

---

**Last Updated:** December 2024
**Status:** âœ… Complete
