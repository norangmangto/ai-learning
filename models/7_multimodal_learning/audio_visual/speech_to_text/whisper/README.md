# Whisper (Speech-to-Text)

OpenAI's robust speech recognition model that handles multiple languages.

## üìã Overview

**Task:** Audio ‚Üí Text transcription
**Languages:** 99 languages supported
**Model Sizes:** Tiny, Base, Small, Medium, Large
**Accuracy:** 95%+ on English, varies by language
**Training Data:** 680k hours multilingual audio

## üéØ Key Features

```
Robust to:
‚úì Background noise
‚úì Technical language
‚úì Accents and dialects
‚úì Multiple languages in same audio
‚úì Music, laughter, etc.

Handles:
‚úì Timestamps
‚úì Language identification
‚úì Task selection (transcribe vs translate)
‚úì No need for fine-tuning
```

## üöÄ Quick Start

```python
import whisper

# Load model
model = whisper.load_model("base")  # or "tiny", "small", "medium", "large"

# Transcribe
result = model.transcribe("audio.mp3")

# Results
print(result["text"])  # Full transcription
print(result["language"])  # Detected language

# Detailed output
for segment in result["segments"]:
    print(f"{segment['start']:.1f}s - {segment['end']:.1f}s: {segment['text']}")
```

## üìä Model Sizes and Performance

```
Model       Size    VRAM    Speed    Accuracy
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
tiny        39M     ~1GB    50√ó      55%
base        74M     ~1GB    32√ó      85%
small       244M    ~2GB    16√ó      92%
medium      769M    ~5GB    8√ó       96%
large       1550M   ~10GB   4√ó       99%

Speed = relative to real-time (16√ó = 16√ó faster than audio)
Accuracy = on test set (English)
```

## üéØ Model Selection

```
Choose model based on:

Available compute?
‚îú‚îÄ GPU with 1GB ‚Üí "tiny" or "base"
‚îú‚îÄ GPU with 5GB ‚Üí "small" or "medium"
‚îî‚îÄ GPU with 10GB+ ‚Üí "large"

Need speed?
‚îú‚îÄ Real-time/low-latency ‚Üí "tiny" or "base"
‚îú‚îÄ Batch processing ‚Üí "medium" or "large"
‚îî‚îÄ Quality critical ‚Üí "large"

Language diversity?
‚îú‚îÄ Mostly English ‚Üí "base"
‚îú‚îÄ Multiple languages ‚Üí "medium" or "large"
‚îî‚îÄ Rare languages ‚Üí "large"

Default recommendation:
"base" or "small" (good balance)
```

## üíª Running Locally

```python
import whisper
import torch

# Use GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
model = whisper.load_model("base", device=device)

# Transcribe
result = model.transcribe("audio.mp3")
```

## üåç Multilingual Support

### Language Identification
```python
result = model.transcribe("audio.mp3")
print(result["language"])  # Detected language (e.g., "fr" for French)
```

### Translate to English
```python
# If audio is in French, translate to English
result = model.transcribe("audio.mp3", task="translate")
# Returns English translation instead of French transcription
```

### Supported Languages
```
99 languages including:
- Major: English, Spanish, French, German, Chinese, Japanese, Korean
- Many others: Arabic, Hindi, Portuguese, Russian, Thai, Vietnamese, etc.

Full list: See Whisper documentation
```

## üìä Audio Preprocessing

```python
import librosa
import numpy as np

# Load audio
audio, sr = librosa.load("audio.mp3", sr=16000)

# Normalization (Whisper expects certain audio characteristics)
if np.max(np.abs(audio)) > 1.0:
    audio = audio / np.max(np.abs(audio))

# Whisper handles most preprocessing automatically
# But good preprocessing can improve results
```

## üéØ Advanced Options

```python
# Verbose output
result = model.transcribe("audio.mp3", verbose=True)

# Word-level timestamps
result = model.transcribe("audio.mp3", language="en")
for segment in result["segments"]:
    for word_info in segment.get("words", []):
        print(f"{word_info['word']}: {word_info['start']:.2f}-{word_info['end']:.2f}s")

# Temperature (controls uncertainty)
# Higher temperature ‚Üí more random
# Lower temperature ‚Üí more confident
result = model.transcribe("audio.mp3", temperature=0.0)  # Most confident
```

## üìà Performance Evaluation

```
Word Error Rate (WER):
WER = (S + D + I) / N
where:
S = substitutions
D = deletions
I = insertions
N = reference words

Example:
Reference: "hello world today"
Hypothesis: "hello world"
WER = 1/3 = 33%

Whisper WER:
- English: ~5-10%
- Spanish: ~10-15%
- Japanese: ~15-20%
- Rare languages: 20-50%+
```

## ‚ö†Ô∏è Common Issues

1. **Audio quality matters**
   ```
   Bad: Heavy background noise, poor microphone
   ‚Üí Higher error rate

   Solution: Pre-process audio, remove noise
   ```

2. **Very long audio**
   ```
   Problem: Might lose context halfway through

   Solution:
   - Use large model for long audio
   - Break into chunks and transcribe separately
   ```

3. **Domain-specific language**
   ```
   Problem: Medical, legal, technical terms
   ‚Üí Lower accuracy

   Solution:
   - No fine-tuning available
   - Post-process with spell checker for domain
   - Use human review for important content
   ```

4. **Cost for large-scale**
   ```
   Problem: Running large model is slow

   Solutions:
   - Use smaller model and accept lower accuracy
   - Batch processing (more efficient)
   - Use quantization (see below)
   ```

## üöÄ Optimization Techniques

### Quantization
```python
import whisper
from transformers import AutoModelForSpeechSeq2Seq
import torch

# Load with lower precision
model = whisper.load_model("base")
model = model.half()  # Float16 instead of Float32
# Half memory, similar accuracy
```

### Batch Processing
```python
audio_files = ["audio1.mp3", "audio2.mp3", ..., "audio100.mp3"]

# More efficient than one-by-one
results = []
for audio_file in audio_files:
    result = model.transcribe(audio_file)
    results.append(result)
```

### Streaming (with Faster-Whisper)
```python
from faster_whisper import WhisperModel

# Faster inference than official Whisper
model = WhisperModel("base", device="cuda", compute_type="float16")

# Can do streaming transcription
segments, info = model.transcribe("audio.mp3")
for segment in segments:
    print(f"[{segment.start:.2f}s] {segment.text}")
```

## üìà Applications

| Domain | Use Case |
|--------|----------|
| **Transcription** | Podcast transcripts, meetings |
| **Accessibility** | Captions for videos |
| **Search** | Audio search (speech to text) |
| **Customer service** | Call transcription |
| **Accessibility** | Speech-to-text for deaf |
| **Translation** | Audio translation to English |
| **Data preparation** | Label audio data for training |

## üí° Comparison with Alternatives

| Method | Accuracy | Speed | Cost | Requires Model |
|--------|----------|-------|------|---|
| Whisper (large) | 99% | Slow | Free | 1.5GB |
| Whisper (base) | 85% | Fast | Free | 74MB |
| Google Speech API | 98% | Fast | $ | Cloud |
| AWS Transcribe | 96% | Fast | $ | Cloud |
| OpenAI API | 99% | Fast | $ | Cloud |

## üéì Learning Outcomes

- [x] Whisper model architecture
- [x] Language identification and translation
- [x] Model selection for your use case
- [x] Audio preprocessing
- [x] Performance evaluation
- [x] Optimization techniques

## üìö Resources

- **Whisper**: https://github.com/openai/whisper
- **Faster-Whisper**: https://github.com/guillaumekln/faster-whisper
- **OpenAI Documentation**: https://platform.openai.com/docs/guides/speech-to-text

## üí° Production Checklist

```
‚úì Choose model size (base or small recommended)
‚úì Test on your audio domain
‚úì Set up error handling (network, audio errors)
‚úì Implement caching (avoid re-transcribing)
‚úì Set up monitoring (accuracy, latency)
‚úì Plan for updates (new Whisper versions)
‚úì Consider API vs local (cost vs latency)
‚úì Pre-process audio if needed (denoise, trim)
‚úì Post-process output (fix common errors)
‚úì Test with various audio qualities
```

---

**Last Updated:** December 2024
**Status:** ‚úÖ Complete
