# Encoder-Decoder Transformer (Seq2Seq)

Original transformer architecture for sequence-to-sequence tasks like translation.

## ğŸ“‹ Overview

**Architecture:** Encoder + Decoder with cross-attention
**Masking:** Encoder (none), Decoder (causal)
**Best For:** Translation, summarization, seq2seq

## ğŸ—ï¸ Architecture

```
Source Language (English):
[e_1 e_2 e_3 e_4] â†’ Encoder â†’ [c_1 c_2 c_3 c_4]
                     (bidirectional)

Target Language (French):
Decoder
[<START>] â†’ Cross-Attention to encoder â†’ f_1
[<START> f_1] â†’ Cross-Attention to encoder â†’ f_2
...
```

## ğŸ¯ Key Features

### Encoder
- Processes entire source sequence
- Bidirectional self-attention
- Outputs context vectors for each position

### Decoder
- Generates target token-by-token
- Causal self-attention (can't see future)
- Cross-attention to encoder outputs
- Attends to relevant source positions

### Cross-Attention
```
Query: Decoder state (what am I generating?)
Key/Value: Encoder outputs (what source info is available?)

â†’ Allows decoder to focus on relevant parts of input
```

## ğŸ“Š Information Flow

```
English: "The cat sat on the mat"
          [h_1 h_2 h_3 h_4 h_5 h_6]

French: "Le"  â†’ Attend to [The], [cat] â†’ "chat"
        "chat" â†’ Attend to [cat], [sat] â†’ "Ã©tait"
        "Ã©tait" â†’ Attend to [sat], [on] â†’ "assis"
        "assis" â†’ Attend to [on], [the], [mat] â†’ "sur"
        ...

Cross-attention weights visualize alignment!
```

## ğŸš€ Quick Start

```python
from train_pytorch import Transformer

# Create model
model = Transformer(
    src_vocab_size=100,
    tgt_vocab_size=80,
    d_model=256,
    num_encoder_layers=3,
    num_decoder_layers=3,
    num_heads=8,
    d_ff=1024
)

# Training
src, tgt = batch
logits, _, _ = model(src, tgt)
loss = CrossEntropyLoss(logits.view(-1, tgt_vocab_size),
                        targets.view(-1))

# Inference: greedy decoding
def translate(source):
    encoder_output = model.encode(source)
    current = [<START>]
    while len(current) < max_len:
        logits = model.decode(current, encoder_output)
        next_token = logits[-1, :].argmax()
        current.append(next_token)
        if next_token == <END>:
            break
    return current
```

## ğŸ“ˆ Applications

| Task | Example |
|------|---------|
| **Machine Translation** | English â†’ French |
| **Summarization** | Long text â†’ Short summary |
| **Paraphrase** | Text â†’ Rephrase |
| **Simplification** | Complex â†’ Simple |
| **Code Generation** | Natural language â†’ Code |

## ğŸ’¡ Encoder-Decoder Intuition

```
ENCODER: "What's in the input?"
         â†“ Process and understand
         [Context vectors for each position]

DECODER: "What should I generate next?"
         â†“ Cross-attention to encoder
         [Focus on relevant input parts]
         â†“
         [Generate one token at a time]
```

## ğŸ”„ Training vs Inference

### Training (Teacher Forcing)
```python
# Use ground truth targets as decoder input
decoder_input = target_tokens[:-1]  # All but last
decoder_output = model.decode(decoder_input, encoder_output)
loss = criterion(decoder_output, target_tokens[1:])  # All but first
```

**Advantage:** Faster training (parallel)
**Disadvantage:** Exposure bias (model trained on gold, tests on own outputs)

### Inference (Autoregressive)
```python
# Use model's own outputs as next input
output = [<START>]
while output[-1] != <END>:
    decoder_output = model.decode(output, encoder_output)
    next_token = decoder_output[-1, :].argmax()
    output.append(next_token)
return output
```

**Advantage:** Real test scenario
**Disadvantage:** Slower (sequential)

## âš ï¸ Common Issues

1. **Exposure Bias**: Train with gold, test with predictions
   - Solution: Scheduled sampling
2. **Length Mismatch**: Model over/under-generates
   - Solution: Length penalty in decoding
3. **Repetition**: Generates same tokens repeatedly
   - Solution: Coverage mechanism
4. **Slow Inference**: Greedy decoding is slow
   - Solution: Beam search, caching

## ğŸ“ Learning Outcomes

- [x] Encoder-decoder architecture
- [x] Cross-attention mechanism
- [x] Teacher forcing training
- [x] Autoregressive decoding
- [x] Translation quality metrics

## ğŸ“š Key Papers

- **Original**: "Attention Is All You Need" (Vaswani et al., 2017)
- **Beam Search**: "Effective Approaches to Attention" (Luong et al., 2015)
- **Coverage**: "Addressing the Rare Word Problem" (Luong et al., 2014)

## ğŸ“Š Improvements Over Simpler Seq2Seq

| Aspect | Simple Seq2Seq | Transformer Seq2Seq |
|--------|---|---|
| Parallelization | Limited | Full âœ… |
| Long Sequences | Poor | Excellent âœ… |
| Interpretability | Limited | Good (attention) âœ… |
| Speed | Slow | Fast âœ… |
| Accuracy | Medium | High âœ… |

---

**Last Updated:** December 2024
**Status:** âœ… Complete
