# Vision Transformer (ViT)

Transformer architecture adapted for image classification via patch embedding.

## ğŸ“‹ Overview

**Architecture:** Image patching + Transformer encoder
**Input:** Images (any resolution)
**Output:** Class logits
**Best For:** Image classification, transfer learning

## ğŸ—ï¸ Architecture

```
Image (H Ã— W Ã— C)
    â†“
Patch Embedding (16Ã—16 patches)
    â†“
[CLS token] + [Patch embeddings] + [Position embeddings]
    â†“
Transformer Encoder (12-24 layers)
    â†“
[CLS token output] â†’ MLP Head
    â†“
Class logits
```

## ğŸ¯ Key Insight

Instead of convolutions, split image into patches and treat as sequence!

```
256Ã—256 image with 16Ã—16 patches = 256 patches
Like text: "This is a cat" â†’ [This, is, a, cat]
Like image: [patch_1, patch_2, ..., patch_256]
```

## ğŸ“ Patch Embedding

### Process

```python
# Input image: (3, 224, 224)
# Patch size: 16Ã—16

patches = unfold(image, patch_size=16)
# Output: (49, 768)  # 14Ã—14=196 patches (in typical ViT)
```

### Formula
```
P = (H / patch_size) Ã— (W / patch_size)
D = C Ã— patch_sizeÂ²

For ViT-Base (224Ã—224, 16Ã—16 patches):
P = 14 Ã— 14 = 196 patches
D = 3 Ã— 16Â² = 768 dimensions
```

## ğŸ§  Architecture Components

### 1. Patch Embedding Layer
```
[Image] â†’ Conv(kernel=16, stride=16) â†’ [Embedded patches]
```
Each patch becomes a high-dimensional vector.

### 2. Position Embedding
```
[CLS] + [Patch1] + [Patch2] + ... + [Patch196]
  +        +          +              +
[Pos0] + [Pos1] + [Pos2] + ... + [Pos196]
```
Tells transformer which patches are adjacent.

### 3. Transformer Encoder
```
Input: [CLS, P1, P2, ..., P196]
  â†“
Multi-head self-attention (cross-patch attention)
  â†“
Each patch attends to all other patches!
  â†“
Output: [CLS, P1', P2', ..., P196']
```

### 4. Classification Head
```
[CLS] token (special token)
  â†“
Linear â†’ Softmax
  â†“
Class probabilities
```

## ğŸ” Self-Attention in Vision

```
Query each patch: "What other patches are relevant?"
  â†“
Attention weights show spatial relationships:
- Patches of the same object â†’ High attention
- Distant patches â†’ Lower attention

Example: Cat detection
- Ear patch attends to: eye, face, other ear patches
- Whisker patch attends to: mouth, face patches
- Tail patch might attend to: body patches
```

## ğŸ“Š Information Flow

```
Image: [background, cat_head, cat_body, cat_tail]
         â†“
Patches: [P1, P2, P3, ..., P196]
         â†“
Layer 1: Each patch learns simple features
         P1 sees: nearby edge patterns

Layer 6: Mid-level representations
         P1, P2 interact heavily (connected object)

Layer 12: High-level semantics
         "cat" emerges from collective patch understanding
```

## ğŸš€ Quick Start

```python
from train_pytorch import VisionTransformer
import torch

# Create model
model = VisionTransformer(
    img_size=224,
    patch_size=16,
    in_channels=3,
    num_classes=1000,
    d_model=768,
    num_heads=12,
    num_layers=12,
    d_ff=3072,
    dropout=0.1
)

# Forward pass
images = torch.randn(32, 3, 224, 224)  # Batch of 32 images
logits = model(images)  # (32, 1000)
predictions = logits.argmax(dim=1)

# Training
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = torch.nn.CrossEntropyLoss()

for images, labels in train_loader:
    logits = model(images)
    loss = criterion(logits, labels)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

## ğŸ“ˆ Applications

| Task | Use Case |
|------|----------|
| **Classification** | ImageNet, CIFAR-10 |
| **Detection** | Object detection (+ decoder) |
| **Segmentation** | Semantic, instance segmentation |
| **Transfer Learning** | Pretrain on ImageNet, finetune |
| **Multimodal** | Vision-language (+ text encoder) |

## âœ¨ ViT Variants

| Variant | Size | Params | Speed | Accuracy |
|---------|------|--------|-------|----------|
| ViT-Tiny | 192 | 5M | âš¡âš¡âš¡ | 72% |
| ViT-Small | 384 | 22M | âš¡âš¡ | 81% |
| ViT-Base | 768 | 86M | âš¡ | 84% |
| ViT-Large | 1024 | 307M | ğŸŒ | 87% |
| ViT-Huge | 1280 | 632M | ğŸ¢ | 88% |

## ğŸ’¡ Why ViT Works

```
CNN advantage: Local receptive field (inductive bias)
             â†’ Good for small images

ViT advantage: Global self-attention
             â†’ Learns what to attend to
             â†’ Better with lots of data
             â†’ More scalable

Trade-off:
- ViTs need more data than CNNs
- ViTs scale better (can be huge)
- ViTs are more interpretable (attention maps)
```

## ğŸ“ Position Embedding Visualization

```
Without position embedding:
[patch_1, patch_2, patch_3] = [patch_3, patch_1, patch_2]
(Model doesn't know spatial order!)

With position embedding:
[patch_1 + pos_1, patch_2 + pos_2, patch_3 + pos_3]
(Model knows spatial locations)
```

## ğŸ“Š Attention Visualization

```
Image: Cat on floor
       [Head] [Body] [Tail]
         P1    P2     P3

Layer 1 (low-level):
P1 â†’ mostly attends to P1 (local features)
P2 â†’ mostly attends to P2
P3 â†’ mostly attends to P3

Layer 6 (mid-level):
P1 â†’ also attends to P2 (connected object)
P2 â†” P1, P3 (part of same animal)
P3 â†’ attends to P1, P2 (connected)

Layer 12 (semantic):
All patches share information
Collective "cat" understanding
```

## âš ï¸ Key Differences from CNNs

| Aspect | CNN | ViT |
|--------|-----|-----|
| **Receptive field** | Grows with layers | Global from start |
| **Inductive bias** | Locality, translation | None (learned) |
| **Data needed** | Medium (1M images) | High (14M+ images) |
| **Interpretability** | Learned filters | Attention maps |
| **Efficiency** | Fast (local ops) | Slower (global ops) |
| **Scalability** | Limited | Excellent |

## ğŸ”„ Training Tips

1. **Large datasets preferred**
   - ViT-Base needs 14M+ images (ImageNet-21K)
   - Smaller datasets: use pretrained ViT

2. **Patch size matters**
   - Smaller patches: more tokens (slower, better detail)
   - Larger patches: fewer tokens (faster, less detail)
   - Default 16Ã—16 is usually optimal

3. **Resolution handling**
   - ViTs can handle variable resolutions
   - Standard training: 224Ã—224
   - Higher resolution fine-tuning: 384Ã—384, 512Ã—512

4. **Computational cost**
   - Attention: O(nÂ²) where n = number of patches
   - 196 patches = manageable
   - 1024 patches = slow (unless efficient attention)

## ğŸ“ Learning Outcomes

- [x] Image-to-sequence transformation (patching)
- [x] Position embeddings for spatial information
- [x] Self-attention on image regions
- [x] Why ViTs outperform CNNs at scale
- [x] Attention visualization for interpretability

## ğŸ“š Key Papers

- **ViT**: "An Image Is Worth 16Ã—16 Words" (Dosovitskiy et al., 2020)
- **DeiT**: "Training Data-Efficient ViTs" (Touvron et al., 2021)
- **Swin**: "Shifted Windows ViT" (Liu et al., 2021)

## ğŸ“Š ViT vs CNN Performance

```
ImageNet Accuracy (100 epoch training):

ViT-B + 14M image pretrain:    84.6%
ResNet-50:                     76.1%
ResNet-101:                    79.8%

ViT advantage grows with:
- More training data
- Larger model scale
- Transfer learning tasks
```

## ğŸ’ª Advantages

âœ… **Scalability** - Works with massive models (1B+ params)
âœ… **Interpretability** - Attention maps show what it sees
âœ… **Versatility** - Same architecture for many tasks
âœ… **Transfer learning** - Excellent pretrained models

## ğŸš¨ Disadvantages

âŒ **Data hungry** - Needs lots of images
âŒ **Slower inference** - Quadratic attention complexity
âŒ **Less inductive bias** - Need more parameters for small data
âŒ **Higher latency** - For real-time applications

---

**Last Updated:** December 2024
**Status:** âœ… Complete
