# RNN Variants: LSTM, GRU, Bidirectional, and Attention

Recurrent Neural Networks (RNNs) process sequential data by maintaining hidden state across time steps. This module covers the main RNN architectures and variants.

## ğŸ“‹ Overview

| Type | Release | Gates | Best For |
|------|---------|-------|----------|
| **Vanilla RNN** | 1997 | None | Educational |
| **LSTM** | 1997 | 3 (input, forget, output) | Long-term dependencies âœ… |
| **GRU** | 2014 | 2 (reset, update) | Efficient LSTM |
| **Bidirectional** | 1997 | Bi-directional processing | Understanding tasks |
| **Attention** | 2015 | Attention mechanism | Focus on relevant steps |

## ğŸ—ï¸ Common Architecture

All RNNs follow the same principle:

```
h_t = f(x_t, h_{t-1})
y_t = g(h_t)
```

Where:
- `x_t`: Input at time step t
- `h_t`: Hidden state (memory)
- `y_t`: Output at time step t
- `f`: RNN function (different for each variant)

## 1ï¸âƒ£ LSTM (Long Short-Term Memory)

### Problem with Vanilla RNN

```
Gradient Flow Issue:
    â†“ â†“ â†“ â†“ â†“ â†“ â†“ â†“ â†“ â†“
h_0 â†’ h_1 â†’ h_2 â†’ ... â†’ h_T

Vanishing Gradients:
âˆ‚L/âˆ‚h_0 = âˆ‚L/âˆ‚h_T Ã— âˆ‚h_T/âˆ‚h_{T-1} Ã— ... Ã— âˆ‚h_1/âˆ‚h_0
          â†‘ product of small values (< 1) â†’ vanishes
```

### LSTM Solution

**Three Gates Control Information Flow:**

```
forget_gate = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)
input_gate = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)
output_gate = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)

candidate = tanh(W_c Â· [h_{t-1}, x_t] + b_c)

C_t = forget_gate Â· C_{t-1} + input_gate Â· candidate
h_t = output_gate Â· tanh(C_t)
```

**Key Components:**

| Gate | Purpose | Formula | Range |
|------|---------|---------|-------|
| **Forget** | Erase old info | Ïƒ(...) | [0,1] |
| **Input** | Add new info | Ïƒ(...) | [0,1] |
| **Output** | Select what to reveal | Ïƒ(...) | [0,1] |
| **Candidate** | What to add | tanh(...) | [-1,1] |

### Cell State vs Hidden State

```
Cell State (C_t):
â”œâ”€ Memory: accumulates information
â”œâ”€ Additive updates (forget + input)
â”œâ”€ Constant error flow (multiplication by forget gate)
â””â”€ Prevents vanishing gradients

Hidden State (h_t):
â”œâ”€ Output: what's visible to next layer
â”œâ”€ Modulated by output gate
â””â”€ Passed to next time step and final layer
```

### Unrolled LSTM Over Time

```
t=0          t=1          t=2
x_0          x_1          x_2
 â†“           â†“            â†“
[LSTM]      [LSTM]       [LSTM]
 â†“ â†“          â†“ â†“          â†“ â†“
h_0 C_0     h_1 C_1      h_2 C_2
 â†“           â†“            â†“
y_0          y_1          y_2
```

### Advantages
âœ… Solves vanishing gradient problem
âœ… Long-term dependency learning
âœ… Industry standard
âœ… Well-understood

### Disadvantages
âŒ Complex (7 matrix multiplications)
âŒ Computationally expensive
âŒ More parameters to tune

---

## 2ï¸âƒ£ GRU (Gated Recurrent Unit)

### Simplified LSTM

**Two Gates Instead of Three:**

```
reset_gate = Ïƒ(W_r Â· [h_{t-1}, x_t] + b_r)
update_gate = Ïƒ(W_u Â· [h_{t-1}, x_t] + b_u)

candidate = tanh(W_h Â· [reset_gate Â· h_{t-1}, x_t] + b_h)
h_t = (1 - update_gate) Â· h_{t-1} + update_gate Â· candidate
```

### GRU vs LSTM

| Aspect | LSTM | GRU |
|--------|------|-----|
| Gates | 3 | 2 |
| Cell State | Yes (separate) | No |
| Parameters | More | Fewer (3/4 of LSTM) |
| Training Speed | Slower | Faster âœ… |
| Performance | Better on long sequences | Comparable on medium |
| Interpretability | Output gate | Simpler |

### Gate Intuition

```
Reset Gate:
â”œâ”€ Controls how much previous state to remember
â”œâ”€ If 0: ignore previous state
â””â”€ If 1: use all of previous state

Update Gate:
â”œâ”€ Controls how much to update state
â”œâ”€ If 0: keep previous state
â””â”€ If 1: use candidate (like LSTM input gate)
```

### Advantages
âœ… Fewer parameters than LSTM
âœ… Faster training
âœ… Comparable performance
âœ… Simpler to understand

### Disadvantages
âŒ Slightly less expressive than LSTM
âŒ No separate cell state (less flexibility)
âŒ Not always better than LSTM

---

## 3ï¸âƒ£ Bidirectional RNNs

### Processing Direction

**Unidirectional (Left-to-Right):**
```
x_0 â†’ h_0 â†’
x_1 â†’ h_1 â†’
x_2 â†’ h_2 â†’ ...
```
Only past context

**Bidirectional (Both Directions):**
```
         â†’ h_f_0 â†’
x_0 âŸ²
         â† h_b_0 â†

    concat(h_f, h_b) = context from both directions
```

### Architecture

```
Input Sequence: [x_0, x_1, x_2, x_3]

Forward Pass:
x_0 â†’ LSTM â†’ h_f_0 â†˜
x_1 â†’ LSTM â†’ h_f_1 â†˜
x_2 â†’ LSTM â†’ h_f_2 â†˜
x_3 â†’ LSTM â†’ h_f_3 â†˜

Backward Pass:
          â†™ h_b_0 â† LSTM â† x_0
          â†™ h_b_1 â† LSTM â† x_1
          â†™ h_b_2 â† LSTM â† x_2
          â†™ h_b_3 â† LSTM â† x_3

Output: [h_f_0 âŠ• h_b_0, h_f_1 âŠ• h_b_1, h_f_2 âŠ• h_b_2, h_f_3 âŠ• h_b_3]
where âŠ• = concatenation
```

### Information Aggregation

**Different pooling strategies for final representation:**

```python
# Last hidden state
output = hidden_states[-1]

# Mean pooling
output = hidden_states.mean(dim=0)

# Max pooling
output = hidden_states.max(dim=0)

# Attention pooling
output = sum(attention_weights * hidden_states)
```

### Use Cases

**Understanding vs Generation:**
- **Bidirectional**: Classification, tagging, understanding âœ…
- **Unidirectional**: Generation, sequence prediction âœ…

### Advantages
âœ… Full context (past and future)
âœ… Better for classification
âœ… Works for understanding tasks
âœ… Improved accuracy vs unidirectional

### Disadvantages
âŒ Can't generate (needs future)
âŒ Slower inference (need full sequence)
âŒ Double parameters
âŒ 2Ã— memory usage

---

## 4ï¸âƒ£ Attention Mechanism in RNNs

### Attention Problem

**Without Attention:**
```
Encoder compresses sequence into single vector
x_1, x_2, x_3, x_4 â†’ LSTM â†’ [final hidden state]
                          â†‘ information bottleneck
                    loses info about x_1, x_2
```

**With Attention:**
```
Encoder produces sequence of states
x_1, x_2, x_3, x_4 â†’ LSTM â†’ [h_1, h_2, h_3, h_4]
                             â†‘ maintain all information
Decoder can focus on relevant states at each step
```

### Attention Types

**1. Additive Attention (Bahdanau):**
```
score(s_t, h_i) = v^T Â· tanh(W Â· [s_t; h_i])
```
General purpose, slower

**2. Multiplicative Attention (Luong):**
```
score(s_t, h_i) = s_t^T Â· W Â· h_i
```
Faster, requires matching dimensions

**3. Scaled Dot-Product:**
```
score(s_t, h_i) = (s_t^T Â· h_i) / sqrt(d_k)
```
Used in Transformers, numerically stable

**4. Self-Attention:**
```
Sequence attends to itself
h_i = sum_j attention(h_i, h_j) Â· h_j
```

### Computation Steps

```
1. Query: Current state s_t (what am I looking for?)
2. Keys: Encoder states h_i (what info is available?)
3. Values: Encoder states h_i (what info to aggregate?)

scores = Attention(Query, Keys) â†’ [length]
         â†“
weights = softmax(scores) â†’ probabilities over positions
         â†“
context = sum(weights * Values) â†’ weighted information
         â†“
output = combine(state, context) â†’ updated representation
```

### Attention Visualization

```
                    weights for position t:
Decoder at t_3:     [0.02, 0.85, 0.10, 0.03]
                     â†“    â†“    â†“    â†“
Encoder:     [h_0  h_1  h_2  h_3]
                     â†‘ focus here

context = 0.02*h_0 + 0.85*h_1 + 0.10*h_2 + 0.03*h_3
```

### Use Cases

- Seq2Seq (Machine Translation)
- Question Answering
- Attention-based Caption Generation
- Visual Question Answering

### Advantages
âœ… Interpretability (see what model attends to)
âœ… Better handling of long sequences
âœ… Improves accuracy significantly
âœ… Foundation for Transformers

### Disadvantages
âŒ Added complexity
âŒ More parameters
âŒ Computation: O(seq_lenÂ²)

---

## ğŸ“Š Comparison: LSTM vs GRU vs Attention-based

| Metric | LSTM | GRU | Attention |
|--------|------|-----|-----------|
| Parameters | Many | Medium | Many |
| Speed | Medium | Fast âœ… | Slow |
| Accuracy | High âœ… | Medium | High âœ… |
| Long Sequences | Good âœ… | Good | Best âœ… |
| Interpretability | Medium | Medium | High âœ… |
| Complexity | High | Medium | Very High |

---

## ğŸ¯ When to Use What

```
Task: Classification
â”œâ”€ Use: Bidirectional LSTM
â”œâ”€ Why: Full context, standard approach
â””â”€ Example: Sentiment analysis

Task: Sequence Generation
â”œâ”€ Use: LSTM or GRU (unidirectional)
â”œâ”€ Why: Can only use past context
â””â”€ Example: Text generation

Task: Long Sequences (> 100 steps)
â”œâ”€ Use: Attention-based or Transformer
â”œâ”€ Why: Solves vanishing gradient better
â””â”€ Example: Machine translation

Task: Fast Training, Limited Data
â”œâ”€ Use: GRU
â”œâ”€ Why: Fewer parameters, faster training
â””â”€ Example: Small dataset NLP
```

---

## ğŸ’¡ Implementation Tricks

### Gradient Clipping
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```
Prevents exploding gradients in RNNs

### Dropout
```python
# Varies per timestep, consistent across RNN
rnn = nn.LSTM(..., dropout=0.5)
```

### Learning Rate
```python
# RNNs are sensitive to learning rate
optimizer = Adam(model.parameters(), lr=0.001)  # Often need smaller LR
```

### Weight Initialization
```python
# Orthogonal initialization helps gradient flow
nn.init.orthogonal_(rnn.weight_hh_l0)
```

---

## ğŸš€ Training Tips

1. **Start with GRU**: Faster, comparable performance
2. **Use Bidirectional for Classification**: Better accuracy
3. **Add Attention for Long Sequences**: Improves results
4. **Monitor Gradients**: RNNs gradient flow is critical
5. **Use Gradient Clipping**: Essential for stability

---

## ğŸ“š Resources

### Key Papers
- **LSTM**: "Long Short-Term Memory" (Hochreiter & Schmidhuber, 1997)
- **GRU**: "Learning Phrase Representations with RNNs" (Cho et al., 2014)
- **Attention**: "Neural Machine Translation with Attention" (Bahdanau et al., 2015)
- **Self-Attention**: "Attention Is All You Need" (Vaswani et al., 2017)

### Intuitions
- **Forget Gate**: "Do I need to remember this?"
- **Input Gate**: "Is this information important?"
- **Output Gate**: "Should I reveal this state?"

---

## âš ï¸ Common Issues

1. **Vanishing Gradients**: Use LSTM/GRU
2. **Exploding Gradients**: Use gradient clipping
3. **Slow on Long Sequences**: Add attention
4. **Poor Convergence**: Reduce learning rate, use warm-up
5. **Overfitting**: Add dropout

---

**Last Updated:** December 2024
**Status:** âœ… Complete with 5 RNN variant implementations
