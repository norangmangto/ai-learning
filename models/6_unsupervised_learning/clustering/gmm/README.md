# Gaussian Mixture Model (GMM)

Probabilistic clustering using mixture of Gaussian distributions.

## ğŸ“‹ Overview

**Type:** Probabilistic, soft clustering
**Clusters:** K (specified)
**Algorithm:** Expectation-Maximization (EM)
**Best For:** Soft assignments, uncertainty quantification

## ğŸ¯ Core Idea

Instead of hard clusters, each point has probability of belonging to each cluster.

```
K-Means (hard):              GMM (soft):
Point â†’ Cluster 1            Point â†’ 60% Cluster 1
        100% certain              â†’ 30% Cluster 2
        0% Cluster 2             â†’ 10% Cluster 3

More realistic! Points on cluster boundary have uncertainty.
```

## ğŸ“ Mathematical Foundation

### Mixture Model
$$p(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)$$

Where:
- $\pi_k$ = mixture weight (prior probability)
- $\mathcal{N}(x | \mu_k, \Sigma_k)$ = Gaussian with mean $\mu_k$, covariance $\Sigma_k$

### Gaussian Distribution
$$\mathcal{N}(x | \mu, \Sigma) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$$

## ğŸ”„ EM Algorithm

### E-step: Responsibilities
```
For each point, compute probability of being in each cluster:

Î³_k(x) = (Ï€_k * N(x | Î¼_k, Î£_k)) / Î£_j(Ï€_j * N(x | Î¼_j, Î£_j))

Higher responsibility = more likely point belongs to cluster k
```

### M-step: Update Parameters
```
Update cluster parameters based on responsibilities:

Ï€_k â† (1/N) Î£_i Î³_k(x_i)
Î¼_k â† Î£_i Î³_k(x_i) * x_i / Î£_i Î³_k(x_i)
Î£_k â† Î£_i Î³_k(x_i) * (x_i - Î¼_k)(x_i - Î¼_k)^T / Î£_i Î³_k(x_i)
```

## ğŸ“Š Iteration Visualization

```
Initial: Random clusters

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â—       â”‚ Gaussian 1
â”‚ â—  â—    â”‚ Gaussian 2
â”‚    â—    â”‚ Gaussian 3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After E-step:
Points have soft assignments to clusters

After M-step:
Gaussian parameters update based on responsibilities

After 10 iterations:
Well-fit Gaussians to data
```

## ğŸš€ Quick Start

```python
from sklearn.mixture import GaussianMixture
import numpy as np

# Data
X = np.random.randn(300, 2)

# Fit GMM
gmm = GaussianMixture(n_components=3, random_state=42)
gmm.fit(X)

# Soft assignments (responsibilities)
soft_labels = gmm.predict_proba(X)
# Shape: (300, 3), each row sums to 1

# Hard assignments (highest responsibility)
hard_labels = gmm.predict(X)

# Model parameters
means = gmm.means_  # (3, 2)
covariances = gmm.covariances_  # (3, 2, 2)
weights = gmm.weights_  # (3,)

# Likelihood
log_likelihood = gmm.score(X)

# BIC/AIC for model selection
bic = gmm.bic(X)
aic = gmm.aic(X)

# Generate samples
samples = gmm.sample(n_samples=100)
```

## ğŸ“Š Covariance Types

```
Different covariance structures:

'full': Î£_k unrestricted
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â—â—    â”‚ Elliptical, any orientation
â”‚ â—  â—   â”‚
â”‚  â—     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

'tied': Î£_k = Î£ (shared covariance)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â—â—    â”‚ Same shape/size for all
â”‚ â—  â—   â”‚ clusters
â”‚  â—     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

'diag': Diagonal covariance (no correlation)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â—â—    â”‚ Axis-aligned ellipses
â”‚ â—  â—   â”‚
â”‚  â—     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

'spherical': Î£_k = Ïƒ_kÂ²I (circles)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â—â—    â”‚ Circular clusters
â”‚ â—  â—   â”‚
â”‚  â—     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Choosing Number of Components

### Method 1: BIC/AIC
```python
components_range = range(1, 10)
bic_scores = []
aic_scores = []

for n in components_range:
    gmm = GaussianMixture(n_components=n)
    gmm.fit(X)
    bic_scores.append(gmm.bic(X))
    aic_scores.append(gmm.aic(X))

# Lower is better
optimal_k = components_range[np.argmin(bic_scores)]
```

### Method 2: Silhouette Score
```python
from sklearn.metrics import silhouette_score

scores = []
for k in range(2, 10):
    gmm = GaussianMixture(n_components=k)
    labels = gmm.fit_predict(X)
    score = silhouette_score(X, labels)
    scores.append(score)

optimal_k = np.argmax(scores) + 2
```

## ğŸ’¡ GMM vs K-Means

```
Point in cluster boundary:

K-Means: 100% Cluster A, 0% Cluster B
         (hard, unrealistic)

GMM: 55% Cluster A, 45% Cluster B
     (soft, captures uncertainty)

When to use each:
- Hard assignments needed â†’ K-Means
- Uncertainty matters â†’ GMM
- Probability distribution needed â†’ GMM
- Speed critical â†’ K-Means
- Well-separated clusters â†’ K-Means
- Overlapping clusters â†’ GMM
```

## ğŸ“ˆ Applications

| Domain | Use Case |
|--------|----------|
| **Finance** | Portfolio clustering with uncertainty |
| **Biology** | Gene expression soft clusters |
| **Speech** | GMM-HMM for speech recognition |
| **Anomaly** | Likelihood-based outlier detection |
| **Vision** | Soft image segmentation |

## âš ï¸ Common Issues

1. **Singularity**
   - Covariance becomes singular (non-invertible)
   - Solution: Add regularization (`reg_covar=1e-6`)

2. **Wrong K**
   - Use BIC/AIC for model selection
   - Solution: Systematically test multiple K

3. **Slow convergence**
   - Many iterations needed
   - Solution: Increase `max_iter` or use `n_init=10`

4. **Local optima**
   - EM can get stuck locally
   - Solution: Try multiple initializations (`n_init=10`)

## ğŸ“ Learning Outcomes

- [x] Mixture model concept
- [x] EM algorithm (E-step, M-step)
- [x] Soft vs hard clustering
- [x] Covariance matrix types
- [x] Model selection (BIC/AIC)

## ğŸ“š Key Papers

- **Original**: "Maximum Likelihood Estimation" (Dempster et al., 1977)
- **GMM**: "Mixture Models" (McLachlan & Peel, 2000)

## ğŸ’ª Advantages

âœ… **Probabilistic** - Principled framework with likelihoods
âœ… **Soft assignments** - Uncertainty quantification
âœ… **Flexible** - Different covariance types
âœ… **Model selection** - BIC/AIC for choosing K
âœ… **Generative** - Can sample from model

## ğŸš¨ Disadvantages

âŒ **Slower** - EM iterations vs K-means
âŒ **Singularity issues** - Can fail with high dimensions
âŒ **Assumes Gaussians** - Poor for non-Gaussian data
âŒ **More parameters** - Covariance matrices to estimate

---

**Last Updated:** December 2024
**Status:** âœ… Complete
