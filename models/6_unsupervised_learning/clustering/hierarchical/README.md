# Hierarchical Clustering

Build a hierarchy of clusters through agglomerative or divisive approach.

## ğŸ“‹ Overview

**Type:** Agglomerative (bottom-up)
**Dendrogram:** Visual tree of clusters
**Complexity:** O(nÂ²) to O(nÂ³)
**Best For:** Understanding cluster relationships, variable number of clusters

## ğŸ—ï¸ Agglomerative Approach

### Algorithm
```
1. Start: Each point is its own cluster
   {1}, {2}, {3}, {4}, {5}

2. Merge closest pair
   {1,2}, {3}, {4}, {5}

3. Merge closest pair
   {1,2}, {3,4}, {5}

4. Merge closest pair
   {1,2}, {3,4,5}

5. Continue until one cluster
   {1,2,3,4,5}
```

### Distance Metrics

**Single Linkage** (Minimum distance)
```
d({A}, {B}) = min(d(a,b)) for aâˆˆA, bâˆˆB
              â”‚
              â””â”€ Connects closest points
```
âš ï¸ Forms chains (not ideal)

**Complete Linkage** (Maximum distance)
```
d({A}, {B}) = max(d(a,b)) for aâˆˆA, bâˆˆB
              â”‚
              â””â”€ Connects farthest points
```
âœ… Compact, well-separated clusters

**Average Linkage** (Average distance)
```
d({A}, {B}) = mean(d(a,b)) for aâˆˆA, bâˆˆB
              â”‚
              â””â”€ Balanced approach
```
âœ… Most popular

**Ward Linkage** (Minimize variance)
```
d({A}, {B}) = increase in sum of squared distances
              when merging A and B
              â”‚
              â””â”€ Matches K-means criteria
```
âœ… Produces compact clusters

## ğŸ“Š Dendrogram Visualization

```
Height (distance between clusters)
    â”‚
5.0 â”œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
    â”‚     â”‚     â”‚
4.0 â”‚   â”Œâ”€â”´â”€â”   â”‚
    â”‚   â”‚   â”‚   â”‚
3.0 â”‚ â”Œâ”€â”´â”€â” â””â”€â”¬â”€â”˜
    â”‚ â”‚   â”‚   â”‚
2.0 â”œâ”€â”´â”¬â”€â”â”œâ”€â”€â”€â”´â”€â”€
    â”‚ â”‚ â”‚ â”‚â”‚
1.0 â”‚ â”‚ â”‚ â”‚â”‚
    â””â”€â”´â”€â”´â”€â”´â”˜
     1 3 5 2 4    â† Points

Reading dendrogram:
- Horizontal distance = dissimilarity
- Cut at h=2.5 â†’ 3 clusters: {1,3}, {5,2}, {4}
- Cut at h=1.5 â†’ 5 clusters: {1}, {3}, {5}, {2}, {4}
```

## ğŸš€ Quick Start

```python
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
import numpy as np

# Data
X = np.random.randn(100, 2)

# Hierarchical clustering
clusterer = AgglomerativeClustering(
    n_clusters=3,
    linkage='ward'  # or 'complete', 'average', 'single'
)
labels = clusterer.fit_predict(X)

# Dendrogram
Z = linkage(X, method='ward')
dendrogram(Z)
plt.axhline(y=threshold, color='r', linestyle='--')
plt.show()

# Cut dendrogram at specific height
from scipy.cluster.hierarchy import fcluster
cluster_labels = fcluster(Z, t=threshold, criterion='distance')
```

## ğŸ” Choosing Linkage Method

```
Decision tree:

Want compact clusters?
â”œâ”€ Yes â†’ Ward linkage âœ… (matches K-means)
â””â”€ No  â†’ Average linkage

Allow chains?
â”œâ”€ Yes â†’ Single linkage (rare, chaining issues)
â””â”€ No  â†’ Complete linkage (too restrictive)

Default recommendation: Ward
```

## ğŸ“ˆ Single vs Complete Linkage

```
Complete Linkage (worst-case distance):
    â—           â—
  â—   â—       â—   â—
Merges when farthest pair is close
â†’ Well-separated, roughly equal-sized

Single Linkage (best-case distance):
    â—           â—
  â—   â—       â—   â—
Merges when closest pair is close
â†’ Forms long chains (not ideal for most cases)
```

## ğŸ“Š Choosing Number of Clusters

### Method 1: Dendrogram Visual Inspection
```
         â†‘ Large gaps in dendrogram
         â”‚ = good cut points
     â”Œâ”€â”€â”€â”´â”€â”€â”€â”
     â”‚       â”‚
   â”Œâ”€â”´â”€â”   â”Œâ”€â”´â”€â”
   â”‚   â”‚   â”‚   â”‚  â† Cut here for K=2, 3, or 4
```

### Method 2: Distance Threshold
```python
# Cut dendrogram at distance threshold
cluster_labels = fcluster(Z, t=5.0, criterion='distance')
n_clusters = len(np.unique(cluster_labels))
```

### Method 3: Elbow Method
```python
# Last K merges show largest distance increases
last_k = 10
last_distances = Z[-last_k:, 2]
plt.plot(last_distances)  # Look for elbow
```

## ğŸ’¡ Dendrogram Interpretation

```
Dendrogram for customer segmentation:

Height
    â”‚
    â”œâ”€ Large jump â† Different segments!
    â”‚
    â”œâ”€ Small jumps â† Similar customers within segment
    â”‚

Indicates:
- Clear 2-3 customer segments
- No clear 10-cluster structure
```

## âš ï¸ Limitations

1. **Cannot handle large datasets**
   - O(nÂ²) to O(nÂ³) complexity
   - Solution: Mini-batch approximations

2. **Hard to choose K**
   - Must cut dendrogram at some height
   - Somewhat subjective

3. **Irreversible**
   - Once merged, clusters can't be split
   - Solution: Agglomerative always better than divisive

4. **Sensitive to outliers**
   - Can affect linkage distances
   - Solution: Remove outliers or use robust distances

## ğŸ¯ Applications

| Domain | Use Case |
|--------|----------|
| **Gene sequencing** | Phylogenetic trees |
| **Social networks** | Community detection |
| **Customer segments** | Understanding relationships |
| **Image segmentation** | Hierarchical regions |
| **Taxonomy** | Biological classification |

## ğŸ“Š Hierarchical vs Flat Clustering

| Aspect | Hierarchical | K-Means |
|--------|-----------|---------|
| **Structure** | Tree (dendrogram) | Flat (K clusters) |
| **K needed** | No (can cut anywhere) | Yes |
| **Scalability** | Poor (O(nÂ²)) | Excellent (O(nk)) |
| **Interpretability** | Good (see relationships) | Simple |
| **Speed** | Slow | Fast |

## ğŸ”„ Divisive Approach (Top-Down)

```
Rare, but exists:

1. Start with all points in one cluster
2. Split into 2 clusters
3. Recursively split until single points
4. Build tree from top-down

Why rare?
- More expensive (exponential splits)
- No clear split criterion
- Less useful for most applications
```

## ğŸ“ Learning Outcomes

- [x] Agglomerative hierarchical clustering
- [x] Different linkage methods
- [x] Dendrogram interpretation
- [x] How to choose number of clusters
- [x] Pros and cons vs K-means

## ğŸ“š Key Papers

- **Original**: "The structure of a cluster" (Jardine & Sibson, 1971)
- **Ward Linkage**: "Hierarchical Grouping for Optimization" (Ward, 1963)

## ğŸ’ª Advantages

âœ… **No K needed** - Choose clusters from dendrogram
âœ… **Interpretable** - See hierarchical relationships
âœ… **Deterministic** - Same result every run
âœ… **Versatile** - Multiple linkage options

## ğŸš¨ Disadvantages

âŒ **Slow** - O(nÂ²) or O(nÂ³) complexity
âŒ **Memory intensive** - Stores all distances
âŒ **Irreversible** - Bad early merges cannot be undone
âŒ **Not for big data** - Limited to thousands of points

## ğŸ’¡ Real-World Tips

1. **For large datasets**
   - Use K-means or DBSCAN instead
   - Or subsample data for dendrogram

2. **Always visualize dendrogram**
   ```python
   from scipy.cluster.hierarchy import dendrogram
   plt.figure(figsize=(10, 5))
   dendrogram(Z)
   plt.show()
   ```

3. **Use Ward linkage by default**
   - Produces compact, meaningful clusters
   - Matches K-means objective

4. **Cut dendrogram at natural gaps**
   - Look for large jumps in distance
   - Usually indicates true cluster boundaries

---

**Last Updated:** December 2024
**Status:** âœ… Complete
