# DBSCAN Clustering

Density-based clustering that discovers arbitrary-shaped clusters and outliers.

## ðŸ“‹ Overview

**Type:** Density-based
**K required:** No
**Outliers:** Automatic detection
**Complexity:** O(n log n) with spatial indexing
**Best For:** Non-spherical clusters, automatic outlier detection

## ðŸŽ¯ Core Idea

Clusters are dense regions separated by sparse regions.

```
DBSCAN's view:        K-Means's view:

â—â—â—       â—          â—â—â— â—
â— â—   â—â—â—â—â—          â—   â—â—â—â—
â—â—â—   â—â—â—â—           â—â—â— â—â—â—â—
  â—â—â—       â—

Dense regions = clusters
Sparse points = outliers

K-Means forces all into K clusters
DBSCAN finds natural groupings
```

## ðŸ“ Definitions

### Epsilon-neighborhood
$$N_\epsilon(p) = \{q : d(p, q) \leq \epsilon\}$$

All points within distance $\epsilon$ from $p$.

### Core Point
Point $p$ is core if $|N_\epsilon(p)| \geq \text{MinPts}$

Has enough neighbors to define cluster.

### Border Point
Not core, but in $\epsilon$-neighborhood of core point.

### Outlier/Noise Point
Not core and not border point.

## ðŸ”„ Algorithm

```
1. Find all core points
   (points with â‰¥ MinPts neighbors within Îµ)

2. Form clusters by connecting core points
   If two core points are within Îµ, same cluster

3. Add border points to clusters
   Assign to cluster of nearby core point

4. Mark remaining points as outliers
   (noise)
```

## ðŸ“Š Visualization

```
Îµ = radius around each point
MinPts = 3 (minimum neighbors)

Core points (â‰¥3 neighbors):     Border points:        Outliers:
    â—â—â—                              â—¯                    â—‹
  â—   â—â—        vs    â—â—â—â—â—â—    vs    â—¯
    â—â—â—           â—â—â—   â—‹             â—¯

Core can connect! Adds borders!  Isolated points!
```

## ðŸš€ Quick Start

```python
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
import numpy as np

# Data
X = np.random.randn(300, 2)

# Fit DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

# Results
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_outliers = list(labels).count(-1)

print(f"Clusters: {n_clusters}")
print(f"Outliers: {n_outliers}")

# Label -1 = outlier
outlier_mask = labels == -1
```

## ðŸŽ¯ Choosing Parameters

### Epsilon (Îµ)

#### K-distance Graph Method
```python
from sklearn.neighbors import NearestNeighbors

# k-distance where k = MinPts
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(X)
distances, indices = neighbors_fit.kneighbors(X)
distances = np.sort(distances[:, 4], axis=0)

# Plot distances - look for "elbow"
plt.plot(distances)
# Elbow point â‰ˆ Îµ value
plt.show()
```

```
Distance plot:
       â†‘
       â”‚     â•±    â† Outliers (steep rise)
       â”‚   â•±
       â”‚ â•±        â† Elbow here! Îµ â‰ˆ 0.5
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Point index

Outliers cause sharp increase in distance
Elbow marks transition to core/border points
```

#### Distance Distribution
```python
# Calculate distances to k-th nearest neighbor
distances = np.sort(distances[:, 4])

# Visual inspection
plt.hist(distances, bins=50)
plt.xlabel('Distance to 5th neighbor')
# Natural gap = good Îµ threshold
```

### MinPts

**Rule of thumb:**
```
MinPts = 2 Ã— dimensions

For 2D data: MinPts = 4
For 3D data: MinPts = 6
For 10D data: MinPts = 20
```

**Or:** Use k from k-distance graph (typically 4-5)

## ðŸ“Š Parameter Sensitivity

```
Îµ too small:          Îµ too large:
Almost all outliers   Everything one cluster

â—  â—  â—  â—           â—â—â—â—â—â—
â—  â—  â—  â—     vs    â—â—â—â—â—â—
â—  â—  â—  â—           â—â—â—â—â—â—

MinPts too small:     MinPts too large:
Every point core      Almost all outliers

â—â—â—â—â—â—â—              â—  â—  â—  â—
â—â—â—â—â—â—â—       vs     â—  â—  â—  â—
â—â—â—â—â—â—â—              â—  â—  â—  â—
```

## ðŸ’¡ Density Intuition

```
Idea: Clusters are dense, surrounded by sparse regions

Low density cluster:          High density cluster:
  â— â— â— â—                      â—â—â—â—â—â—
  â—   â—         vs             â—â—â—â—â—â—
  â— â— â— â—                      â—â—â—â—â—â—

DBSCAN can find both if Îµ is appropriate!
Advantage over K-Means which forces spherical shapes
```

## ðŸ“ˆ Applications

| Domain | Use Case |
|--------|----------|
| **Spatial data** | Finding geographic clusters |
| **Anomaly** | Automatic outlier detection |
| **Gene expression** | Variable-sized clusters |
| **Traffic** | Congestion regions |
| **Social media** | Community detection |

## ðŸ” DBSCAN vs K-Means

| Aspect | DBSCAN | K-Means |
|--------|--------|---------|
| **K required** | No | Yes |
| **Cluster shape** | Any | Spherical |
| **Outliers** | Automatic | Forced in clusters |
| **Speed** | O(n log n) | O(nk) |
| **Scalability** | Good | Excellent |
| **Parameter tuning** | Medium | Easy |

## ðŸŽ“ Learning Outcomes

- [x] Core, border, noise points
- [x] Epsilon and MinPts parameters
- [x] Parameter selection methods
- [x] Density-based vs partition-based
- [x] Automatic outlier detection

## ðŸ“š Key Papers

- **Original**: "A Density-Based Algorithm for Discovering Clusters" (Ester et al., 1996)

## ðŸ’ª Advantages

âœ… **No K needed** - Automatically determines clusters
âœ… **Any shape** - Finds non-spherical clusters
âœ… **Outlier detection** - Automatic noise identification
âœ… **Scalable** - O(n log n) with spatial indexing
âœ… **Principled** - Density-based, interpretable

## ðŸš¨ Disadvantages

âŒ **Parameter tuning** - Difficult for new datasets
âŒ **Varying densities** - Poor with density variations
âŒ **High dimensions** - Curse of dimensionality
âŒ **Sparse data** - Many outliers detected

## ðŸ’¡ Real-World Tips

1. **Always use k-distance graph**
   ```python
   # Plot distances to see natural Îµ
   neighbors = NearestNeighbors(n_neighbors=5)
   neighbors.fit(X)
   distances, _ = neighbors.kneighbors(X)
   distances = np.sort(distances[:, -1])
   plt.plot(distances)
   ```

2. **Start with MinPts = 2Ã—d**
   ```python
   d = X.shape[1]
   min_pts = 2 * d
   ```

3. **Standardize features**
   ```python
   from sklearn.preprocessing import StandardScaler
   X_scaled = StandardScaler().fit_transform(X)
   ```

4. **Check outlier percentage**
   - 0-5% outliers: reasonable
   - >10% outliers: Îµ might be too small

---

**Last Updated:** December 2024
**Status:** âœ… Complete
